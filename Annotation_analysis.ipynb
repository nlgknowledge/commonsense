{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Annotation analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU6Fp69IQbkO"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13gnjedvP80i"
      },
      "source": [
        "paper_annotation = pd.read_csv('drive/MyDrive/CS_Eval/papers.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHEjLUhuQake"
      },
      "source": [
        "paper_annotation = paper_annotation.iloc[1:] #deleted first row because it included the overall categories, e.g operationalisation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "faoWkth8TLAa",
        "outputId": "8ee40fd5-c3f2-4f9a-e991-28ea2ec46174"
      },
      "source": [
        "paper_annotation.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Key</th>\n",
              "      <th>Annotator</th>\n",
              "      <th>Date</th>\n",
              "      <th>Misc comments</th>\n",
              "      <th>Exclusion flag (= TRUE if paper doesn't contain human eval)</th>\n",
              "      <th>Venue</th>\n",
              "      <th>Authors</th>\n",
              "      <th>Year</th>\n",
              "      <th>Definition of \\ncommonsense \\n(copy paste \\nor none)</th>\n",
              "      <th>Type of commonsense (e.g. reasoning, etc.) or none if no further attribute is noted.</th>\n",
              "      <th>Link</th>\n",
              "      <th>Language (full name)</th>\n",
              "      <th>Input</th>\n",
              "      <th>External knowledge?</th>\n",
              "      <th>Output</th>\n",
              "      <th>Task</th>\n",
              "      <th>Was the knowledge evaluated in the generated text?</th>\n",
              "      <th>List or Range of response values</th>\n",
              "      <th>Size of rating instrument</th>\n",
              "      <th>Type of scale or rating instrument</th>\n",
              "      <th>Data type of collected responses</th>\n",
              "      <th>Form of Response Elicitation</th>\n",
              "      <th>Verbatim question/prompt/etc</th>\n",
              "      <th>Statistics computed on response values</th>\n",
              "      <th>Verbatim Criterion Name</th>\n",
              "      <th>Verbatim Definition</th>\n",
              "      <th>Paraphrase of Criterion Name</th>\n",
              "      <th>Criterion name for evaluation of external knowledge (if the external knowledge is not evaluated, leave blank)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Zhang2020</td>\n",
              "      <td>DG</td>\n",
              "      <td>04/01/21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ACL</td>\n",
              "      <td>Zhang et al.</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>none</td>\n",
              "      <td>understanding how concepts are related</td>\n",
              "      <td>https://www.aclweb.org/anthology/2020.acl-main...</td>\n",
              "      <td>English</td>\n",
              "      <td>multiple (list all): text:sentence, text: subs...</td>\n",
              "      <td>ConceptNet, GPT-2</td>\n",
              "      <td>text: sentence</td>\n",
              "      <td>dialogue turn generation</td>\n",
              "      <td>No</td>\n",
              "      <td>1-4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>rank ordering</td>\n",
              "      <td>rank order</td>\n",
              "      <td>relative quality estimation</td>\n",
              "      <td>not given</td>\n",
              "      <td>Fleiss’ Kappa</td>\n",
              "      <td>Appropriateness</td>\n",
              "      <td>Appropriateness evaluates if the response is o...</td>\n",
              "      <td>--------------- 6. Appropriateness (content)</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Zhang2020</td>\n",
              "      <td>DG</td>\n",
              "      <td>04/01/21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ACL</td>\n",
              "      <td>Zhang et al.</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>none</td>\n",
              "      <td>understanding how concepts are related</td>\n",
              "      <td>https://www.aclweb.org/anthology/2020.acl-main...</td>\n",
              "      <td>English</td>\n",
              "      <td>multiple (list all): text:sentence, text: subs...</td>\n",
              "      <td>ConceptNet, GPT-2</td>\n",
              "      <td>text: sentence</td>\n",
              "      <td>dialogue turn generation</td>\n",
              "      <td>No</td>\n",
              "      <td>1-4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>rank ordering</td>\n",
              "      <td>rank order</td>\n",
              "      <td>relative quality estimation</td>\n",
              "      <td>not given</td>\n",
              "      <td>Fleiss’ Kappa</td>\n",
              "      <td>informativeness</td>\n",
              "      <td>informativeness evaluates systems’ ability to ...</td>\n",
              "      <td>---------- 28/29. Information content of outputs</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020.emnlp-main.54</td>\n",
              "      <td>DG</td>\n",
              "      <td>12/01/21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>EMNLP</td>\n",
              "      <td>Ji et al.</td>\n",
              "      <td>2020.0</td>\n",
              "      <td>none</td>\n",
              "      <td>reasoning ability</td>\n",
              "      <td>https://www.aclweb.org/anthology/2020.emnlp-ma...</td>\n",
              "      <td>English</td>\n",
              "      <td>text: multiple sentences</td>\n",
              "      <td>ConceptNet</td>\n",
              "      <td>text: sentence</td>\n",
              "      <td>other (please specify): story ending generation</td>\n",
              "      <td>No</td>\n",
              "      <td>win, tie and lose</td>\n",
              "      <td>3.0</td>\n",
              "      <td>other (please specify): win, tie, lose</td>\n",
              "      <td>ratio-scale</td>\n",
              "      <td>relative quality estimation</td>\n",
              "      <td>not given</td>\n",
              "      <td>Fleiss’ Kappa</td>\n",
              "      <td>Fluency</td>\n",
              "      <td>For fluency, we require the annotators to focu...</td>\n",
              "      <td>59. Multiple (list all): 22. Grammaticality, 4...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Key  ... Criterion name for evaluation of external knowledge (if the external knowledge is not evaluated, leave blank)\n",
              "1           Zhang2020  ...                                                NaN                                                           \n",
              "2           Zhang2020  ...                                                NaN                                                           \n",
              "3  2020.emnlp-main.54  ...                                                NaN                                                           \n",
              "\n",
              "[3 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzujFTKkTQ87"
      },
      "source": [
        "# **Venue/Year: where the annotated papers were published and when**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgEVoAYvRXyI"
      },
      "source": [
        "df_pub_info = paper_annotation[['Key', 'Venue', 'Year']]\n",
        "\n",
        "df_pub_info_cleaned = df_pub_info.drop_duplicates('Key')\n",
        "venue = df_pub_info_cleaned['Venue'].value_counts()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACRbBFwMSW4Z",
        "outputId": "4483f691-1022-47bf-b8ec-c93ef8db13ab"
      },
      "source": [
        "venue"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EMNLP             10\n",
              "EMNLP-IJCNLP       8\n",
              "ACL                7\n",
              "NAACL              5\n",
              "NAACL-HLT          1\n",
              "TACL               1\n",
              "Findings-EMNLP     1\n",
              "SemEval            1\n",
              "Name: Venue, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJPGcrQeTdUJ"
      },
      "source": [
        "year = df_pub_info_cleaned['Year'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYQC2PykTkVK",
        "outputId": "596d8e5a-20ad-4a00-dd8d-1912e197dc47"
      },
      "source": [
        "year"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2019.0    20\n",
              "2020.0    12\n",
              "2018.0     2\n",
              "Name: Year, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3dn8-L5a6vP"
      },
      "source": [
        "# **Definitions and types of commonsense**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbA9e_zoTnCj"
      },
      "source": [
        "def_cs = paper_annotation['Definition of \\ncommonsense \\n(copy paste \\nor none)'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coBuCCZCT8qd",
        "outputId": "652613f1-6e8a-4cc0-f4cc-6863ea734892"
      },
      "source": [
        "def_cs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "none                                                                                                                                                                                               64\n",
              "none                                                                                                                                                                                                2\n",
              "commonsense evidence to justify the prediction of an action, \\nto explain an action                                                                                                                 1\n",
              "Commonsense reasoning, the ability to make acceptable and logical assumptions about ordinary scenes in our daily life                                                                               1\n",
              "Machine common sense, or the knowledge of and ability to reason about an open ended world,                                                                                                          1\n",
              "counterfactual reasoning: the ability to predict causal changes in future events given a counterfactual condition applied to the original chain of events (Goodman, 1947; Bottou et al., 2013).     1\n",
              "Name: Definition of \\ncommonsense \\n(copy paste \\nor none), dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_DDKeFJUKYm"
      },
      "source": [
        "type_cs = paper_annotation['Type of commonsense (e.g. reasoning, etc.) or none if no further attribute is noted. '].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNeLtw0nKUG-"
      },
      "source": [
        "df_pub_info = paper_annotation[['Key', 'Venue', 'Year']]\n",
        "\n",
        "df_pub_info_cleaned = df_pub_info.drop_duplicates('Key')\n",
        "venue = df_pub_info_cleaned['Venue'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "D_wCsBjhI_T4",
        "outputId": "6c69ed06-8587-4d07-dbca-8b0a0a341633"
      },
      "source": [
        "type_cs_info = paper_annotation[['Key','Type of commonsense (e.g. reasoning, etc.) or none if no further attribute is noted. ']]\n",
        "\n",
        "type_cs_info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Key</th>\n",
              "      <th>Type of commonsense (e.g. reasoning, etc.) or none if no further attribute is noted.</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Zhang2020</td>\n",
              "      <td>understanding how concepts are related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Zhang2020</td>\n",
              "      <td>understanding how concepts are related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020.emnlp-main.54</td>\n",
              "      <td>reasoning ability</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020.emnlp-main.54</td>\n",
              "      <td>reasoning ability</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2020.tacl-1.7</td>\n",
              "      <td>causal relationship and temporal order of events</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>N19-1238</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>N19-1238</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>N19-1301</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>N19-1299</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>N19-1382</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Key Type of commonsense (e.g. reasoning, etc.) or none if no further attribute is noted. \n",
              "1            Zhang2020             understanding how concepts are related                                   \n",
              "2            Zhang2020             understanding how concepts are related                                   \n",
              "3   2020.emnlp-main.54                                  reasoning ability                                   \n",
              "4   2020.emnlp-main.54                                  reasoning ability                                   \n",
              "5        2020.tacl-1.7   causal relationship and temporal order of events                                   \n",
              "..                 ...                                                ...                                   \n",
              "66            N19-1238                                       non-specific                                   \n",
              "67            N19-1238                                       non-specific                                   \n",
              "68            N19-1301                                       non-specific                                   \n",
              "69            N19-1299                                       non-specific                                   \n",
              "70            N19-1382                                       non-specific                                   \n",
              "\n",
              "[70 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jdB-P0NYKZgU",
        "outputId": "26477172-612b-4c33-e262-7128a4573eef"
      },
      "source": [
        "type_cs_info_cleaned = type_cs_info.drop_duplicates('Key')\n",
        "type_cs_info_cleaned"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Key</th>\n",
              "      <th>Type of commonsense (e.g. reasoning, etc.) or none if no further attribute is noted.</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Zhang2020</td>\n",
              "      <td>understanding how concepts are related</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020.emnlp-main.54</td>\n",
              "      <td>reasoning ability</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2020.tacl-1.7</td>\n",
              "      <td>causal relationship and temporal order of events</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>findings-emnlp.165</td>\n",
              "      <td>1) relational reasoning with background common...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2020.acl-main.711</td>\n",
              "      <td>sarcasm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>P19-1193</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2020.acl-main.515</td>\n",
              "      <td>facts</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>P19-1470</td>\n",
              "      <td>events</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2020.emnlp-main.61</td>\n",
              "      <td>social commonsense</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>D19-1270</td>\n",
              "      <td>If-Then commonsense reasoning, Event-Centered ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>D18-1454</td>\n",
              "      <td>multi-hop relational commonsense information, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2020.findings-emnlp.369</td>\n",
              "      <td>general commonsense QA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>N19-1421</td>\n",
              "      <td>commonsense question answering, commonsense kn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>2020.findings-emnlp.90</td>\n",
              "      <td>commonsense reasoning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>D18-1283</td>\n",
              "      <td>commonsense evidence, common ground, commonsen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>2020.emnlp-main.739</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>2020.semeval-1.39</td>\n",
              "      <td>daily common sense</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>D19-1509</td>\n",
              "      <td>Counterfactual reasoning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>D19-1243</td>\n",
              "      <td>Contextual Commonsense Reasoning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>2020.acl-main.508</td>\n",
              "      <td>commonsense knowledge, commonsense reasoning, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>P19-1488</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>D19-1172</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>D19-1183</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>D19-1189</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>D19-1194</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>D19-1247</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>D19-1255</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>D19-1281</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>D19-1299</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>N19-1126</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>N19-1238</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>N19-1301</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>N19-1299</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>N19-1382</td>\n",
              "      <td>non-specific</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Key Type of commonsense (e.g. reasoning, etc.) or none if no further attribute is noted. \n",
              "1                 Zhang2020             understanding how concepts are related                                   \n",
              "3        2020.emnlp-main.54                                  reasoning ability                                   \n",
              "5             2020.tacl-1.7   causal relationship and temporal order of events                                   \n",
              "7        findings-emnlp.165  1) relational reasoning with background common...                                   \n",
              "8         2020.acl-main.711                                            sarcasm                                   \n",
              "12                 P19-1193                                       non-specific                                   \n",
              "16        2020.acl-main.515                                              facts                                   \n",
              "18                 P19-1470                                             events                                   \n",
              "19       2020.emnlp-main.61                                 social commonsense                                   \n",
              "22                 D19-1270  If-Then commonsense reasoning, Event-Centered ...                                   \n",
              "26                 D18-1454  multi-hop relational commonsense information, ...                                   \n",
              "29  2020.findings-emnlp.369                             general commonsense QA                                   \n",
              "31                 N19-1421  commonsense question answering, commonsense kn...                                   \n",
              "32   2020.findings-emnlp.90                              commonsense reasoning                                   \n",
              "36                 D18-1283  commonsense evidence, common ground, commonsen...                                   \n",
              "37      2020.emnlp-main.739                                       non-specific                                   \n",
              "41        2020.semeval-1.39                                 daily common sense                                   \n",
              "44                 D19-1509                           Counterfactual reasoning                                   \n",
              "45                 D19-1243                   Contextual Commonsense Reasoning                                   \n",
              "50        2020.acl-main.508  commonsense knowledge, commonsense reasoning, ...                                   \n",
              "51                 P19-1488                                       non-specific                                   \n",
              "52                 D19-1172                                       non-specific                                   \n",
              "53                 D19-1183                                       non-specific                                   \n",
              "54                 D19-1189                                       non-specific                                   \n",
              "55                 D19-1194                                       non-specific                                   \n",
              "57                 D19-1247                                       non-specific                                   \n",
              "58                 D19-1255                                       non-specific                                   \n",
              "60                 D19-1281                                       non-specific                                   \n",
              "61                 D19-1299                                       non-specific                                   \n",
              "62                 N19-1126                                       non-specific                                   \n",
              "64                 N19-1238                                       non-specific                                   \n",
              "68                 N19-1301                                       non-specific                                   \n",
              "69                 N19-1299                                       non-specific                                   \n",
              "70                 N19-1382                                       non-specific                                   "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9Qqixl6Kp-S",
        "outputId": "77c03e5c-a661-41b2-c3d3-a0bd0968acbe"
      },
      "source": [
        "\n",
        "type_commonsense_ = type_cs_info_cleaned['Type of commonsense (e.g. reasoning, etc.) or none if no further attribute is noted. '].value_counts()\n",
        "type_commonsense_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "non-specific                                                                                                         16\n",
              "If-Then commonsense reasoning, Event-Centered Commonsense\\r\\nReasoning\\r                                              1\n",
              "daily common sense                                                                                                    1\n",
              "1) relational reasoning with background commonsense knowledge, and 2) compositional generalization                    1\n",
              "commonsense knowledge, commonsense reasoning, the property of entities, temporal knowledge, or spatial knowledge      1\n",
              "commonsense question answering, commonsense knowledge, commonsense reasoning,                                         1\n",
              "general commonsense QA                                                                                                1\n",
              "multi-hop relational commonsense information, Commonsense/Background Knowledge, grounded commonsense (background)     1\n",
              "reasoning ability                                                                                                     1\n",
              "commonsense reasoning                                                                                                 1\n",
              "social commonsense                                                                                                    1\n",
              "understanding how concepts are related                                                                                1\n",
              "sarcasm                                                                                                               1\n",
              "facts                                                                                                                 1\n",
              "Counterfactual reasoning                                                                                              1\n",
              "Contextual Commonsense Reasoning                                                                                      1\n",
              "commonsense evidence, common ground, commonsense knowledge                                                            1\n",
              "causal relationship and temporal order of events                                                                      1\n",
              "events                                                                                                                1\n",
              "Name: Type of commonsense (e.g. reasoning, etc.) or none if no further attribute is noted. , dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLwqgfOtbGm7"
      },
      "source": [
        "# **System language, input, output and task**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv1wz8BRUpde"
      },
      "source": [
        "df_system_info = paper_annotation[['Key', 'Language (full name)', 'Input', 'Output', 'Task']]\n",
        "\n",
        "df_system_info_cleaned = df_system_info.drop_duplicates('Key')\n",
        "\n",
        "language = df_system_info_cleaned['Language (full name)'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVBrcxeEbMPa",
        "outputId": "3cb095a4-7309-4d63-ebac-6a2d3c3c653d"
      },
      "source": [
        "language"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "English             30\n",
              "Englsih              2\n",
              "English, Chinese     1\n",
              "Chinese, English     1\n",
              "Name: Language (full name), dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPFbcDMpUzcj"
      },
      "source": [
        "input = df_system_info_cleaned['Input'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mvJ1IurbQER",
        "outputId": "477481e8-d518-4e53-c3e6-fab1e6fa6e46"
      },
      "source": [
        "input"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text: sentence                                                                           9\n",
              "text: multiple sentences                                                                 6\n",
              "raw/structured data                                                                      6\n",
              "text: subsentential units of text                                                        3\n",
              "visual                                                                                   2\n",
              "text: other (please specify): \\n a short story and a\\r\\ncounterfactual context           1\n",
              "multiple (list all): multiple sentences (paragraph), Question-Answers, Reasoning type    1\n",
              "text: dialogue                                                                           1\n",
              "multiple (list all): text:sentence, text: subsentential unit of text                     1\n",
              "text: documents                                                                          1\n",
              "other (please specify): multiple choices                                                 1\n",
              "unclear (rare)                                                                           1\n",
              "deep linguistic representation (DLR)                                                     1\n",
              "Name: Input, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR4zSG75U5nv"
      },
      "source": [
        "output = df_system_info_cleaned['Output'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDtcjOKNbTBO",
        "outputId": "ff6dc0de-4484-448f-a787-c18df591e3cf"
      },
      "source": [
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text: sentence                                                                                             17\n",
              "text: subsentential units of text                                                                           4\n",
              "text: multiple sentences                                                                                    3\n",
              "raw/structured data                                                                                         2\n",
              "text: variable-length                                                                                       2\n",
              "other (please specify):  a short story (left column) and a\\r\\ncounterfactual context                        1\n",
              "other (please specify): classificated answers                                                               1\n",
              "text: dialogue                                                                                              1\n",
              "text: documents                                                                                             1\n",
              "unclear (rare)                                                                                              1\n",
              "text: other (please specify): multiple-choice questions with corresponding relevant context (snippets)      1\n",
              "Name: Output, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pCdTO72U6vM"
      },
      "source": [
        "task = df_system_info_cleaned['Task'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M783CWVIbV7b",
        "outputId": "9cfb584a-ccec-485e-9196-c41f99143453"
      },
      "source": [
        "task"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "question answering                                 12\n",
              "dialogue turn generation                            7\n",
              "end-to-end text generation                          3\n",
              "feature-controlled generation                       2\n",
              "other (please specify): story ending generation     2\n",
              "content selection/determination                     2\n",
              "question generation                                 1\n",
              "summarisation (text-to-text)                        1\n",
              "data-to-text generation                             1\n",
              "surface realisation (SLR to text)                   1\n",
              "other (please specify): sarcasm generation          1\n",
              "content ordering/structuring                        1\n",
              "Name: Task, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeB3Hs3zbd8o"
      },
      "source": [
        "# **External knowledge: Was it used, was it evaluated and if yes how**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t39a2IvjUzqg",
        "outputId": "221a52d4-ef97-464c-b5d1-6a868af635de"
      },
      "source": [
        "df_external_knowledge = paper_annotation[['External knowledge? ', 'Key']]\n",
        "df_external_knowledge[\"External knowledge? \"] = df_external_knowledge[\"External knowledge? \"].str.replace(',', '')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF68YOr4bn4g"
      },
      "source": [
        "df_cleaned_external = df_external_knowledge.drop_duplicates('Key')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZInW9Cposz2"
      },
      "source": [
        "external_knowledge = df_cleaned_external['External knowledge? '].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8wC1sago5bf",
        "outputId": "319ac256-8274-4d9c-cc58-484b97cc08cf"
      },
      "source": [
        "df_cleaned_external['External knowledge? '].str.split(expand=True).stack().value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConceptNet    13\n",
              "Yes           12\n",
              "GPT-2          7\n",
              "ATOMIC         5\n",
              "No             2\n",
              "COMET          1\n",
              "Cosmos         1\n",
              "graph          1\n",
              "own            1\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmrJWnuUplEK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0roUfs1jU7t5"
      },
      "source": [
        "knowledge_eval = paper_annotation[['Key','Was the knowledge evaluated in the generated text?  ']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTjiIMecbZD5"
      },
      "source": [
        "knowledge_eval_clean = knowledge_eval.drop_duplicates('Key')\n",
        "knowledge_eval_info = knowledge_eval_clean['Was the knowledge evaluated in the generated text?  '].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWOsZJw8brqX",
        "outputId": "0759d864-61e6-4fc6-e4d8-d05c3da07d49"
      },
      "source": [
        "knowledge_eval_info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "No     20\n",
              "Yes    14\n",
              "Name: Was the knowledge evaluated in the generated text?  , dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTQGAlMdanGB"
      },
      "source": [
        "external_knowledge_eval = paper_annotation['Criterion name for evaluation of external knowledge (if the external knowledge is not evaluated, leave blank)'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtzYRKgObxTK",
        "outputId": "693fcad4-a1cc-4dd1-840b-b8e296dd96d0"
      },
      "source": [
        "external_knowledge_eval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "knowledge types                                                                                           1\n",
              "External commonsense knowledge                                                                            1\n",
              "the paper uses plausibility as a criterion to evaluate commonsense knowledge produced by their model.     1\n",
              "Validiy                                                                                                   1\n",
              "Relevance                                                                                                 1\n",
              "Commonsense relations                                                                                     1\n",
              "persona grounding performance with expansions                                                             1\n",
              "Name: Criterion name for evaluation of external knowledge (if the external knowledge is not evaluated, leave blank), dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfnaVhArcbfd"
      },
      "source": [
        "# **Operationalisation: Response values, Ratings, Type of Scales, Data types, form of response elicitation and Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-YtOfZaU8eZ"
      },
      "source": [
        "response_values = paper_annotation['List or Range of response values'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1biUtWocsFZ",
        "outputId": "cd44b5a1-6f85-4624-d171-4adf56608169"
      },
      "source": [
        "response_values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1-5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  12\n",
              "win, tie and lose                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     6\n",
              "1,2,3,4,5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             2\n",
              "R1 is better, Both have similar fluency, R1 is worse                                                                                                                                                                                                                                                                                                                                                                                                                                                                  2\n",
              "1-4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   2\n",
              "0,1,2,3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               2\n",
              "\"win\",\"tie\",\"lose\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    2\n",
              " 2 (Not at all) to 5 (Very)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1\n",
              "0 The reason is not grammatically correct, or not comprehensible at all, or not related to the statement at all.\\n1 The reason is just the negation of the statement or a simple paraphrase. Obviously, a better explanation can be made\\n2 The reason is relevant and appropriate, though it may contain a few grammatical errors or unnecessary parts. Or like case 1, but it’s hard to write a proper reason.\\n3 The reason is appropriate and is a solid explanation of why the statement does not make sense     1\n",
              "R1 is more engaging, Both have similar engagement level, R1 is less engaging                                                                                                                                                                                                                                                                                                                                                                                                                                          1\n",
              "0,1,2,3,4,5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1\n",
              "(1) Does the rewritten ending keep in mind details of the original premise sentence? \\n(2) Is the plot of the rewritten ending relevant to the plot of the original ending? \\n(3) Does the rewritten ending respect the changes induced by the counterfactual sentence?                                                                                                                                                                                                                                               1\n",
              "definitely left, \\nrather left, \\ndifficult to say, \\nrather right, \\ndefinitely right                                                                                                                                                                                                                                                                                                                                                                                                                                1\n",
              "systems 1-6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1\n",
              "Plausible/not plausible                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               1\n",
              "1,2,3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 1\n",
              "Best\", \"Worst\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1\n",
              " 1 (Not at all) to 5 (Very)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1\n",
              "Yes, No, Uncertain                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    1\n",
              "1 = Nonsensical \\n2 = Ambiguous or unanswerable \\n3 = Minor errors (e.g., grammar)\\n4 = Coherent and Fluent                                                                                                                                                                                                                                                                                                                                                                                                           1\n",
              "Name: List or Range of response values, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKymS07tWEnh"
      },
      "source": [
        "rating_instrument_size = paper_annotation['Size of rating instrument'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TblVQ0hncxFM",
        "outputId": "f488afb0-56f6-4d1a-ee01-5710be5aaae0"
      },
      "source": [
        "rating_instrument_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.0      14\n",
              "3.0      13\n",
              "100.0     6\n",
              "4.0       4\n",
              "30.0      3\n",
              "50.0      3\n",
              "20.0      2\n",
              "600.0     1\n",
              "2.0       1\n",
              "6.0       1\n",
              "Name: Size of rating instrument, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMjYPdMcWEzK"
      },
      "source": [
        "type_of_scales = paper_annotation['Type of scale or rating instrument'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivve6BBtczvJ",
        "outputId": "b167be57-2639-4fc4-9055-14b07a1a155d"
      },
      "source": [
        "type_of_scales"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numerical rating scale                    12\n",
              "rank ordering                              9\n",
              "Likert scale                               7\n",
              "other (please specify): win, tie, lose     6\n",
              "output classification                      6\n",
              "zero-centered rating scale                 4\n",
              "counting                                   2\n",
              "unclear                                    2\n",
              "text annotation                            1\n",
              "Name: Type of scale or rating instrument, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8N07JsRWYQ3"
      },
      "source": [
        "data_type_responses = paper_annotation['Data type of collected responses'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgkjQfxJc1--",
        "outputId": "6dd6865c-f92a-4ca9-ba7b-878ce34e2534"
      },
      "source": [
        "data_type_responses"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ordinal        18\n",
              "rank order      9\n",
              "ratio-scale     9\n",
              "categorical     7\n",
              "count           5\n",
              "unclear         2\n",
              "Name: Data type of collected responses, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0RD8xa0WE9y"
      },
      "source": [
        "response_elic_form = paper_annotation['Form of Response Elicitation'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNdgU64Vc5hm",
        "outputId": "3c22a9f4-b8ff-40b7-b403-b403b31bbc7d"
      },
      "source": [
        "response_elic_form"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "relative quality estimation                   21\n",
              "direct quality estimation                     14\n",
              "unclear                                        7\n",
              "(dis)agreement with quality statement          5\n",
              "evaluation through post-editing/annotation     4\n",
              "task performance measurements                  2\n",
              "classification                                 1\n",
              "Name: Form of Response Elicitation, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiJkpOruWFLj"
      },
      "source": [
        "statistics = paper_annotation['Statistics computed on response values'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP-ak60qc7h8",
        "outputId": "b9090086-99a3-44f8-8a37-ef0b966fa088"
      },
      "source": [
        "statistics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Mean                                                 8\n",
              "Fleiss’ Kappa                                        8\n",
              "Avg scores                                           5\n",
              "percentage                                           5\n",
              "p-value, no test mentioned                           4\n",
              "Pearson correlation for inter-annotator agreement    4\n",
              "standard deviation                                   3\n",
              "Cohen’s kappa                                        3\n",
              "none given                                           2\n",
              "sign test                                            2\n",
              "Kappa                                                2\n",
              "Kendall’s rank correlation coefficient               1\n",
              "kappa coefficient                                    1\n",
              "Precision, Recall, F-score                           1\n",
              "Pitman’s test                                        1\n",
              "none                                                 1\n",
              "Percision                                            1\n",
              "Name: Statistics computed on response values, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yRk_auohc0t"
      },
      "source": [
        "verbatim_questions = paper_annotation['Verbatim question/prompt/etc'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke-AtH-qiMYQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "360123e3-659e-4081-c60e-3c9fc97d53c7"
      },
      "source": [
        "verbatim_questions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "not given                                                                                                                                                                                                                                                                  30\n",
              "were the commonsense relations\\nprovided by our algorithm relevant to the question?                                                                                                                                                                                         1\n",
              "Which response do you think is more engaging/interesting?                                                                                                                                                                                                                   1\n",
              "How funny are the sentences?                                                                                                                                                                                                                                                1\n",
              "(1) Does the rewritten ending keep in mind details of the original premise sentence? \\n(2) Is the plot of the rewritten ending relevant to the plot of the original ending? \\n(3) Does the rewritten ending respect the changes induced by the counterfactual sentence?     1\n",
              "Read the attributes, understand them and judge their relevance with regards to their relevance to their video by providing a rating for each                                                                                                                                1\n",
              "Experts are asked to vote for if a generation is fluent for each generated target                                                                                                                                                                                           1\n",
              "Which comment is more fluent?                                                                                                                                                                                                                                               1\n",
              "Experts are asked to give a 1-5 score for the diversity of generations                                                                                                                                                                                                      1\n",
              "whether they can determine the most plausible correct answer                                                                                                                                                                                                                1\n",
              "was any external commonsense knowledge necessary for answering the\\nquestion?                                                                                                                                                                                               1\n",
              "How valid are the paths?                                                                                                                                                                                                                                                    1\n",
              "if they can determine the correct answer, whether the answer can\\r\\nbe determined without looking at the paragraph                                                                                                                                                          1\n",
              "How grammatical are the sentences?                                                                                                                                                                                                                                          1\n",
              "Experts are asked to vote for if a generation is coherent for each generated target                                                                                                                                                                                         1\n",
              "What stiry has better logic?                                                                                                                                                                                                                                                1\n",
              "How creative are the utterances ?                                                                                                                                                                                                                                           1\n",
              "What story has better grammar?                                                                                                                                                                                                                                              1\n",
              "How relevant are the paths to the question?                                                                                                                                                                                                                                 1\n",
              "How sarcastic are the utterances?                                                                                                                                                                                                                                           1\n",
              "Is this following knowledge about B relevant to the given dialogue history?                                                                                                                                                                                                 1\n",
              "Judge the effects with regards to their relevance to their video by providing a rating for each                                                                                                                                                                             1\n",
              "if they can determine the correct answer, whether the answer requires commonsense knowledge                                                                                                                                                                                 1\n",
              "Which response do you think is more relevant to the history?                                                                                                                                                                                                                1\n",
              "Intention of agent's action: judge them in terms of relevance to the videp                                                                                                                                                                                                  1\n",
              "whether the paragraph is inappropriate or nonsensical                                                                                                                                                                                                                       1\n",
              "whether the question is nonsensical or not related to the paragraph                                                                                                                                                                                                         1\n",
              "Name: Verbatim question/prompt/etc, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCtb0ZSudGfF"
      },
      "source": [
        "# **Criterion names and paraphrase**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GerZiTU3aMy3"
      },
      "source": [
        "verbatim_criterion_name = paper_annotation['Verbatim Criterion Name'].str.lower().value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2J6Yde1dMrP",
        "outputId": "a9826d5c-4244-476a-bcee-94f7fcae6de1"
      },
      "source": [
        "verbatim_criterion_name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fluency                                    6\n",
              "coherence                                  4\n",
              "informativeness                            3\n",
              "appropriateness                            2\n",
              "grammaticality                             2\n",
              "diversity                                  2\n",
              "none given                                 2\n",
              "accuracy                                   2\n",
              "correctness                                2\n",
              "validity                                   1\n",
              "common sense reasoning                     1\n",
              "creativity                                 1\n",
              "grammar and fluency                        1\n",
              "event-centered commonsense\\r\\nreasoning    1\n",
              "consistency                                1\n",
              "quality                                    1\n",
              "intention                                  1\n",
              "effectiveness                              1\n",
              "best-worst scaling                         1\n",
              "information                                1\n",
              "attribute                                  1\n",
              "grammatical correctness and fluency        1\n",
              "relevance                                  1\n",
              "logicality                                 1\n",
              "reasonability                              1\n",
              "common ground                              1\n",
              "interpretability                           1\n",
              "common sense                               1\n",
              "humour                                     1\n",
              "none                                       1\n",
              "naturalness                                1\n",
              "usefulness                                 1\n",
              "plausible                                  1\n",
              "topic-consistency                          1\n",
              "commonsense plausibility                   1\n",
              "answerability                              1\n",
              "engagement                                 1\n",
              "sarcasticness                              1\n",
              "novelty                                    1\n",
              "effect                                     1\n",
              "Name: Verbatim Criterion Name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL1J9PV7acGD"
      },
      "source": [
        "paraphrase_criterion_name = paper_annotation['Paraphrase of Criterion Name'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct5I8F6ldQI-",
        "outputId": "4e53a20e-bb03-4c11-e6cb-42f0bbeefdad"
      },
      "source": [
        "paraphrase_criterion_name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "---------- 21. Fluency                                                                  4\n",
              "---------- 9. Coherence                                                                 4\n",
              "---------- 28/29. Information content of outputs                                        4\n",
              "--- 39d. Goodness of outputs relative to input                                          4\n",
              "------ 1. Goodness of outputs relative to input (content)                               4\n",
              "---------- 22. Grammaticality                                                           3\n",
              "--- 15b. Correctness of outputs in their own right                                      2\n",
              "--------------- 34. Naturalness (form)                                                  2\n",
              "------ 16b. Correctness of outputs relative to input (content)                          2\n",
              "--------------- 6. Appropriateness (content)                                            2\n",
              "------ 16c. Correctness of outputs relative to input (both form and content)            2\n",
              "--- 16. Correctness of outputs relative to input                                        1\n",
              "---------- 6b. Appropriateness                                                          1\n",
              "------ 39j. Goodness of outputs relative to system use                                  1\n",
              "--------------- 35a. Naturalness (both form and content)                                1\n",
              "------ 17b. Correctness of outputs relative to external frame of reference (content)    1\n",
              "------ 42a. Goodness of outputs in their own right (both form and content)              1\n",
              "blank                                                                                   1\n",
              "39. Quality of outputs                                                                  1\n",
              "------ 58. Text Property [PROPERTY] (specify): creativity                               1\n",
              "------ 58. Text Property [PROPERTY] (specify): novelty                                  1\n",
              "------ 58. Text Property [PROPERTY] (specify): sarcasticness                            1\n",
              "------ 58. Text Property [PROPERTY] (specify): diversity                                1\n",
              "--- 39c. Goodness of outputs in their own right                                         1\n",
              "--------------- 11. Text Property [Complexity/simplicity (content)]                     1\n",
              "59. Multiple (list all): 22. Grammaticality, 44. Readability                            1\n",
              "------ 58. Text Property [PROPERTY] (specify): humour                                   1\n",
              "--------------- 6a. Appropriateness (both form and content)                             1\n",
              "---------- 12b. Text Property [Complexity/simplicity]                                   1\n",
              "Name: Paraphrase of Criterion Name, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    }
  ]
}